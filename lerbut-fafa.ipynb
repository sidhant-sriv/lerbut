{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral\",  callbacks=CallbackManager([StreamingStdOutCallbackHandler()]),num_gpu=1, base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "print(\"Hello World!\")\n",
      "```"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```python\\nprint(\"Hello World!\")\\n```'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "modelPath = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda:0'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader('mytext', glob='*.txt', loader_cls=TextLoader)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 11           |        cudaMalloc retries: 11        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 660537 KiB |    872 MiB | 158678 MiB | 158033 MiB |\n",
      "|       from large pool | 659840 KiB |    871 MiB | 156571 MiB | 155927 MiB |\n",
      "|       from small pool |    697 KiB |      2 MiB |   2106 MiB |   2105 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 660537 KiB |    872 MiB | 158678 MiB | 158033 MiB |\n",
      "|       from large pool | 659840 KiB |    871 MiB | 156571 MiB | 155927 MiB |\n",
      "|       from small pool |    697 KiB |      2 MiB |   2106 MiB |   2105 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 658325 KiB |    869 MiB | 157964 MiB | 157321 MiB |\n",
      "|       from large pool | 657628 KiB |    868 MiB | 155858 MiB | 155215 MiB |\n",
      "|       from small pool |    697 KiB |      2 MiB |   2106 MiB |   2105 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 735232 KiB |   1280 MiB |   1280 MiB | 575488 KiB |\n",
      "|       from large pool | 733184 KiB |   1276 MiB |   1276 MiB | 573440 KiB |\n",
      "|       from small pool |   2048 KiB |      4 MiB |      4 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  74695 KiB | 248789 KiB |  69987 MiB |  69914 MiB |\n",
      "|       from large pool |  73344 KiB | 247818 KiB |  67879 MiB |  67808 MiB |\n",
      "|       from small pool |   1351 KiB |   2181 KiB |   2107 MiB |   2106 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     285    |     303    |   10973    |   10688    |\n",
      "|       from large pool |     107    |     117    |    6695    |    6588    |\n",
      "|       from small pool |     178    |     191    |    4278    |    4100    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     285    |     303    |   10973    |   10688    |\n",
      "|       from large pool |     107    |     117    |    6695    |    6588    |\n",
      "|       from small pool |     178    |     191    |    4278    |    4100    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      30    |      44    |      44    |      14    |\n",
      "|       from large pool |      29    |      42    |      42    |      13    |\n",
      "|       from small pool |       1    |       2    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      26    |      39    |    6479    |    6453    |\n",
      "|       from large pool |      25    |      34    |    3768    |    3743    |\n",
      "|       from small pool |       1    |       6    |    2711    |    2710    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = './db'\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fanfare]\n",
      "Ladies and gentlemen,\n",
      "for the first time\n",
      "in Piston Cup history...\n",
      "[rewing] A rookie\n",
      "has won the Piston Cup.\n",
      "Yes!\n",
      "[Bob]... we have a three-way tie.\n",
      "[Crowd cheers]\n",
      "[Cameras flash]\n",
      "[Chuckling] Hey,\n",
      "that must be really embarrassing.\n",
      "But I wouldn't worry about it.\n",
      "Because I didn't do it!\n",
      "\n",
      "Based on the context given:\n",
      "\n",
      "* The Piston Cup is being contested by three drivers: The King, Chick Hicks, and Lightning McQueen.\n",
      "* There will be a tiebreaker race between these three drivers to determine the winner of the Piston Cup.\n",
      "* The country has almost shut down in anticipation of this race.\n",
      "* Tickets to the race are hotter than a black leather seat on a hot summer day.\n",
      "* There is a crowd of nearly 200,000 cars at the Los Angeles International Speedway for this race.\n",
      "* Darrell is also present at the race and is excited about it.\n",
      "\n",
      "It is not specified in the context who won the Piston Cup, but it is mentioned that a rookie has won it for the first time.\n",
      "[Fanfare]\n",
      "Ladies and gentlemen,\n",
      "for the first time\n",
      "in Piston Cup history...\n",
      "[rewing] A rookie\n",
      "has won the Piston Cup.\n",
      "Yes!\n",
      "[Bob]... we have a three-way tie.\n",
      "[Crowd cheers]\n",
      "[Cameras flash]\n",
      "[Chuckling] Hey,\n",
      "that must be really embarrassing.\n",
      "But I wouldn't worry about it.\n",
      "Because I didn't do it!\n",
      "\n",
      "Based on the context given:\n",
      "\n",
      "* The Piston Cup is being contested by three drivers: The King, Chick Hicks, and Lightning McQueen.\n",
      "* There will be a tiebreaker race between these three drivers to determine the winner of the Piston Cup.\n",
      "* The country has almost shut down in anticipation of this race.\n",
      "* Tickets to the race are hotter than a black leather seat on a hot summer day.\n",
      "* There is a crowd of nearly 200,000 cars at the Los Angeles International Speedway for this race.\n",
      "* Darrell is also present at the race and is excited about it.\n",
      "\n",
      "It is not specified in the context who won the Piston Cup, but it is mentioned that a rookie has won it for the first time.\n",
      "\n",
      "\n",
      "Sources:\n",
      "mytext/cars-1.txt\n",
      "mytext/cars-1.txt\n",
      "mytext/cars-1.txt\n",
      "mytext/cars-1.txt\n",
      "mytext/cars-1.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"Who won the Piston cup?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
