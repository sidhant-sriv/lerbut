{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ee78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral\",  callbacks=CallbackManager([StreamingStdOutCallbackHandler()]),num_gpu=1, base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda:0'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5098bb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"./data\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2247"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = './db'\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={'k': 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a20b2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "bm25_retriever.k =  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dde6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever],\n",
    "                                       weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ceb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever.get_relevant_documents(\"scholars admitted in 2019, 17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  retriever=ensemble_retriever,\n",
    "                                  return_source_documents=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(query):\n",
    "    llm_response = qa_chain(query)\n",
    "    return llm_response['result']\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I. Eligibility and Registration for Ph.D. Program\n",
      "\n",
      "A. Minimum educational qualification: Master's degree with a minimum of 55% aggregate marks (or equivalent) from a recognized university.\n",
      "B. Application process: Students must submit an application along with required documents to the concerned school, following which they will be called for an interview.\n",
      "C. Registration and fees payment: Upon acceptance into the Ph.D. program, students must register and pay the registration fee within 15 days.\n",
      "\n",
      "II. Minimum Time Duration for Direct Ph.D.\n",
      "\n",
      "The minimum duration for a direct Ph.D. program is three years from the date of registration. However, it's important to note that the actual time taken to complete the research and defend the thesis may vary depending on various factors such as the complexity of the research topic and availability of resources."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I. Eligibility and Registration for Ph.D. Program\\n\\nA. Minimum educational qualification: Master's degree with a minimum of 55% aggregate marks (or equivalent) from a recognized university.\\nB. Application process: Students must submit an application along with required documents to the concerned school, following which they will be called for an interview.\\nC. Registration and fees payment: Upon acceptance into the Ph.D. program, students must register and pay the registration fee within 15 days.\\n\\nII. Minimum Time Duration for Direct Ph.D.\\n\\nThe minimum duration for a direct Ph.D. program is three years from the date of registration. However, it's important to note that the actual time taken to complete the research and defend the thesis may vary depending on various factors such as the complexity of the research topic and availability of resources.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"minimum time duration for direct ph.d\"\n",
    "process_llm_response(query)\n",
    "# qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"VIT QA System\")\n",
    "    user_input = st.text_area(\"Enter Query\")\n",
    "\n",
    "    if st.button(\"Get response\"):\n",
    "        result = process_llm_response(user_input)\n",
    "        st.write(\"Response:\")\n",
    "        # make text bigger\n",
    "        st.write(f\"**{result}**\")\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
