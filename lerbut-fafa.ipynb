{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ee78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral\",  callbacks=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "google_api_key=os.environ.get('PALM')\n",
    "palm.configure(api_key=google_api_key)\n",
    "\n",
    "prompt = 'Explain the difference between effective and affective with examples'\n",
    "\n",
    "llm = GooglePalm(google_api_key=google_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "modelPath = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda:0'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader('mytext', glob='*.txt', loader_cls=TextLoader)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 6            |        cudaMalloc retries: 6         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1390 MiB |   1404 MiB | 109024 MiB | 107633 MiB |\n",
      "|       from large pool |   1387 MiB |   1401 MiB | 107266 MiB | 105878 MiB |\n",
      "|       from small pool |      2 MiB |      3 MiB |   1757 MiB |   1755 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1390 MiB |   1404 MiB | 109024 MiB | 107633 MiB |\n",
      "|       from large pool |   1387 MiB |   1401 MiB | 107266 MiB | 105878 MiB |\n",
      "|       from small pool |      2 MiB |      3 MiB |   1757 MiB |   1755 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1384 MiB |   1400 MiB | 108861 MiB | 107476 MiB |\n",
      "|       from large pool |   1382 MiB |   1397 MiB | 107103 MiB | 105721 MiB |\n",
      "|       from small pool |      2 MiB |      3 MiB |   1757 MiB |   1755 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   1470 MiB |   1470 MiB |   2090 MiB | 634880 KiB |\n",
      "|       from large pool |   1466 MiB |   1466 MiB |   2078 MiB | 626688 KiB |\n",
      "|       from small pool |      4 MiB |      6 MiB |     12 MiB |   8192 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  81616 KiB | 224545 KiB |  47967 MiB |  47887 MiB |\n",
      "|       from large pool |  80000 KiB | 223766 KiB |  46206 MiB |  46128 MiB |\n",
      "|       from small pool |   1616 KiB |   3185 KiB |   1760 MiB |   1759 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     624    |     626    |    9098    |    8474    |\n",
      "|       from large pool |     230    |     230    |    4952    |    4722    |\n",
      "|       from small pool |     394    |     396    |    4146    |    3752    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     624    |     626    |    9098    |    8474    |\n",
      "|       from large pool |     230    |     230    |    4952    |    4722    |\n",
      "|       from small pool |     394    |     396    |    4146    |    3752    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      56    |      56    |      69    |      13    |\n",
      "|       from large pool |      54    |      54    |      63    |       9    |\n",
      "|       from small pool |       2    |       3    |       6    |       4    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      44    |      44    |    5088    |    5044    |\n",
      "|       from large pool |      40    |      40    |    3417    |    3377    |\n",
      "|       from small pool |       4    |       7    |    1671    |    1667    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12071"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "vectordb = None\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = './db'\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bee work camps were shut down, the bees got back their honey, and the glorification of honey was ended.\n",
      "\n",
      "\n",
      "Sources:\n",
      "mytext/bee-movie.txt\n",
      "mytext/bee-movie.txt\n",
      "mytext/bee-movie.txt\n",
      "mytext/bee-movie.txt\n",
      "mytext/bee-movie.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"What happened after bees won the lawsuit?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
